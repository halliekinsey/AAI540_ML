{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Currently if you call a model and its already loaded, it goes through, if the model is not loaded, the model is loaded and then a request gets send to it.\n",
    "## Requests other than the first are quite fast.\n",
    "## Bigger models take much longer\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://100.118.250.126:6000\" # Also worked on: \"http://localhost:6000\"\n",
    "GENERATE_URL = f\"{BASE_URL}/generate\"\n",
    "GENERATE_STREAM_URL = f\"{BASE_URL}/generate_stream\"\n",
    "\n",
    "def generate_text(model_name, text, max_length=100):\n",
    "    \"\"\"\n",
    "    Calls the /generate endpoint and prints the full generated text.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model_name\": model_name,\n",
    "        \"text\": text,\n",
    "        \"max_length\": max_length\n",
    "    }\n",
    "    \n",
    "    response = requests.post(GENERATE_URL, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"\\n=== /generate ===\")\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        print(f\"Prompt: {text}\")\n",
    "        print(\"Generated Text:\")\n",
    "        print(data.get(\"response\", \"No response field in JSON\"))\n",
    "        print(f\"Time taken: {data.get('time_taken', 'N/A')} seconds\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "def generate_text_stream(model_name, text, max_length=100):\n",
    "    \"\"\"\n",
    "    Calls the /generate_stream endpoint and prints tokens as they arrive.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model_name\": model_name,\n",
    "        \"text\": text,\n",
    "        \"max_length\": max_length\n",
    "    }\n",
    "    \n",
    "    response = requests.post(GENERATE_STREAM_URL, json=payload, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"\\n=== /generate_stream ===\")\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        print(f\"Prompt: {text}\")\n",
    "        print(\"Streaming Generated Text:\")\n",
    "        for chunk in response.iter_lines(decode_unicode=True):\n",
    "            if chunk:\n",
    "                print(chunk, end=\" \", flush=True)\n",
    "        print()  # New line after the stream ends\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Example prompts\n",
    "prompt1 = \"Explain the key differences between artificial intelligence and human intelligence.\"  \n",
    "prompt2 = \"What are the potential risks and benefits of artificial general intelligence (AGI)?\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| **Model**                              | **Architecture / Basis**                                             | **Estimated Max Token Limit** | **Estimated Max Word Limit**  |\n",
    "|----------------------------------------|----------------------------------------------------------------------|-------------------------------|-------------------------------|\n",
    "| Equall/Saul-7B-Instruct-v1             | Continued pretraining of Mistral‑7B                                  | ~8,192 tokens                 | ~6,000 words                  |\n",
    "| ricdomolm/lawma-8b                     | Fine‑tuned on Llama‑3 8B Instruct                                    | ~8,192 tokens                 | ~6,000 words                  |\n",
    "| ricdomolm/lawma-70b                    | Fine‑tuned on Llama‑3 70B Instruct for legal classification tasks¹    | ~4,096 tokens                 | ~3,072 words                  |\n",
    "| deepseek‑ai/DeepSeek‑V2‑Lite (and Chat)  | Mixture‑of‑Experts (MoE) model with an extended context window       | 32,000 tokens                 | ~24,000 words                 |\n",
    "\n",
    "\n",
    "\n",
    "¹ Although the base Llama‑3‑70B model supports an 8k-token context, this version was configured for legal classification—where shorter inputs (and corresponding outputs) are sufficient—resulting in an effective limit of approximately 4k tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the code run until this was:\n",
    "\n",
    "```\n",
    "python3 app.py \n",
    " * Serving Flask app 'app'\n",
    " * Debug mode: off\n",
    "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
    " * Running on all addresses (0.0.0.0)\n",
    " * Running on http://127.0.0.1:6000\n",
    " * Running on http://100.118.250.126:6000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Using \"lawma_70b\" model. This is a large model.\n",
    "generate_text(\"lawma_70b\", prompt1, max_length=100)\n",
    "generate_text_stream(\"lawma_70b\", prompt2, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Using \"saul_7b_instruct\" model\n",
    "generate_text(\"saul_7b_instruct\", prompt1, max_length=50)\n",
    "generate_text_stream(\"saul_7b_instruct\", prompt2, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Using \"lawma_8b\" model\n",
    "generate_text(\"lawma_8b\", prompt1, max_length=50)\n",
    "generate_text_stream(\"lawma_8b\", prompt2, max_length=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Using \"DeepSeek-V2-Lite\" model\n",
    "generate_text(\"DeepSeek-V2-Lite\", prompt2, max_length=200)\n",
    "generate_text_stream(\"DeepSeek-V2-Lite\", prompt1, max_length=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
