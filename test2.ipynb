{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Legal Data Retrieval for Compliance Risk Analysis\n",
    "\n",
    "This notebook retrieves legal data from U.S. government APIs, such as govinfo.gov and the Federal Register.\n",
    "The data will be stored in separate directories under `data/` so that it can be processed later in our NLP pipeline.\n",
    "Our objective is to collect as much high-quality data as possible to serve as a baseline for automated identification of compliance risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "These helper functions will ensure directories exist and handle data saving with timestamped filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(directory):\n",
    "    \"\"\"Ensure that a directory exists.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def save_data(output_dir, data, filename_prefix):\n",
    "    \"\"\"Save JSON data to a file with a timestamp in the filename.\"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(output_dir, f\"{filename_prefix}_{timestamp}.json\")\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Data saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data Retrieval Functions\n",
    "\n",
    "Here we define functions to retrieve data from two U.S. sources: govinfo.gov (for U.S. Code, for example) and the Federal Register.\n",
    "We include pagination logic assuming the API supports an offset/limit strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_govinfo_data(api_key, limit=50, max_pages=5):\n",
    "    \"\"\"\n",
    "    Retrieve data from the govinfo.gov API.\n",
    "    \n",
    "    Adjust the endpoint and parameters as required by the API documentation.\n",
    "    This example uses pagination by iterating over pages using an offset.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.govinfo.gov/collections/USCODE\"  # Example endpoint: adjust based on real documentation\n",
    "    all_data = []\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"api_key\": api_key,\n",
    "            \"offset\": page * limit,\n",
    "            \"limit\": limit,\n",
    "        }\n",
    "        try:\n",
    "            print(f\"Fetching govinfo data: Page {page+1}\")\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Assuming the data is in a list under 'results'\n",
    "            if \"results\" in data:\n",
    "                all_data.extend(data[\"results\"])\n",
    "            else:\n",
    "                all_data.append(data)\n",
    "            \n",
    "            # Optional: add sleep to respect API rate limiting\n",
    "            time.sleep(1)  \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error retrieving govinfo data on page {page+1}: {e}\")\n",
    "            break\n",
    "\n",
    "    # Save the aggregated data\n",
    "    save_data(\"data/govinfo\", all_data, \"govinfo_data\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_federal_register_data(limit=50, max_pages=5):\n",
    "    \"\"\"\n",
    "    Retrieve data from the Federal Register API.\n",
    "    \n",
    "    This example uses pagination if supported (offset strategy).\n",
    "    Endpoint and parameters should be adapted according to API documentation.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.federalregister.gov/api/v1/documents.json\"\n",
    "    all_data = []\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"per_page\": limit,\n",
    "            \"page\": page + 1,  # Many APIs use page numbers starting at 1\n",
    "        }\n",
    "        try:\n",
    "            print(f\"Fetching Federal Register data: Page {page+1}\")\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Federal Register API typically returns documents under a key like 'results'\n",
    "            if \"results\" in data:\n",
    "                all_data.extend(data[\"results\"])\n",
    "            else:\n",
    "                all_data.append(data)\n",
    "            \n",
    "            # Optional: add sleep to respect API rate limiting\n",
    "            time.sleep(1)  \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error retrieving Federal Register data on page {page+1}: {e}\")\n",
    "            break\n",
    "            \n",
    "    # Save the aggregated data\n",
    "    save_data(\"data/federal_register\", all_data, \"federal_register_data\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Store U.S. Legal Data\n",
    "\n",
    "Now we call the above functions. You can adjust `max_pages` and `limit` parameters to fetch more data if needed.\n",
    "\n",
    "Ensure you have your API keys and adjust endpoints as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual govinfo API key.\n",
    "govinfo_api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Retrieve U.S. Code data from govinfo.gov API\n",
    "govinfo_results = retrieve_govinfo_data(api_key=govinfo_api_key, limit=50, max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from the Federal Register API\n",
    "federal_register_results = retrieve_federal_register_data(limit=50, max_pages=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the U.S. legal data downloaded and stored in the `data/` directory (split into `govinfo` and `federal_register`), you can now proceed with:\n",
    "\n",
    "- **Data Preprocessing:** Clean and normalize the texts.\n",
    "- **Annotation:** Label sections indicating missing clauses or ambiguous language by integrating with legal expertsâ€™ annotations.\n",
    "- **NLP Modeling:** Fine-tune NLP models (such as Legal-BERT) or apply a RAG framework using this data to support automated identification of compliance risks.\n",
    "\n",
    "This pipeline fits the project goals by collecting the baseline legal corpus for subsequent risk analysis while keeping in mind that it does **not** replace expert legal judgment or generate legal documents from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
